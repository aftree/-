<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>存储 on My New Hugo Site</title>
    <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/</link>
    <description>Recent content in 存储 on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Dec 2019 20:50:40 +0800</lastBuildDate>
    
	<atom:link href="http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/about-storage/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/about-storage/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/cloud-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/cloud-storage/</guid>
      <description>云存储 </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/an-zhuang-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/an-zhuang-2/</guid>
      <description>安装2 这是另一个安装的文章：Ceph分布式存储-运维操作笔记
Centos7下使用Ceph-deploy快速部署Ceph分布式存储-操作记录
1）基本环境
192.168.10.220 ceph-admin（ceph-deploy） mds1、mon1（也可以将monit节点另放一台机器） 192.168.10.239 ceph-node1 osd1 192.168.10.212 ceph-node2 osd2 192.168.10.213 ceph-node3 osd3 ------------------------------------------------- 每个节点修改主机名 # hostnamectl set-hostname ceph-admin # hostnamectl set-hostname ceph-node1 # hostnamectl set-hostname ceph-node2 # hostnamectl set-hostname ceph-node3 ------------------------------------------------- 每个节点绑定主机名映射 # cat /etc/hosts 192.168.10.220 ceph-admin 192.168.10.239 ceph-node1 192.168.10.212 ceph-node2 192.168.10.213 ceph-node3 ------------------------------------------------- 每个节点确认连通性 # ping -c 3 ceph-admin # ping -c 3 ceph-node1 # ping -c 3 ceph-node2 # ping -c 3 ceph-node3 ------------------------------------------------- 每个节点关闭防火墙和selinux # systemctl stop firewalld # systemctl disable firewalld # sed -i &amp;#39;s/SELINUX=enforcing/SELINUX=disabled/g&amp;#39; /etc/selinux/config # setenforce 0 ------------------------------------------------- 每个节点安装和配置NTP（官方推荐的是集群的所有节点全部安装并配置 NTP，需要保证各节点的系统时间一致。没有自己部署ntp服务器，就在线同步NTP） # yum install ntp ntpdate ntp-doc -y # systemctl restart ntpd # systemctl status ntpd ------------------------------------------------- 每个节点准备yum源 删除默认的源，国外的比较慢 # yum clean all # mkdir /mnt/bak # mv /etc/yum.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/an-zhuang/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/an-zhuang/</guid>
      <description>安装 （一个ceph cluster集群，至少需要1个mon节点和2个osd节点）
0）基本信息： 这里我只是测试环境，所以使用一个监控节点，两个存储节点，具体如下： ip地址 主机名 ceph磁盘 备注 192.168.10.200 ceph-node1 20G 作为mds、mon、osd0 192.168.10.201 ceph-node2 20G 作为osd1 192.168.10.202 ceph-node3 20G 作为osd2 192.168.10.203 ceph-client 挂载点：/cephfs ceph客户端 Ceph的文件系统作为一个目录挂载到客户端cephclient的/cephfs目录下，可以像操作普通目录一样对此目录进行操作。 1）安装前准备 分别在ceph的三个节点机（ceph-node1、ceph-node2、ceph-node3）上添加hosts [root@ceph-node1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.10.200 ceph-node1 192.168.10.201 ceph-node2 192.168.10.202 ceph-node3 添加完hosts后，做下测试，保证使用hosts中映射的主机名能ping通。 [root@ceph-node3 ~]# ping ceph-node1 PING ceph-node1 (192.168.10.200) 56(84) bytes of data. 64 bytes from ceph-node1 (192.168.10.200): icmp_seq=1 ttl=64 time=0.211 ms 64 bytes from ceph-node1 (192.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/gai-shu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/gai-shu/</guid>
      <description>概述 1）Ceph简单概述 Ceph是一个分布式存储系统，诞生于2004年，最早致力于开发下一代高性能分布式文件系统的项目。Ceph源码下载：http://ceph.com/download/ 。随着云计算的发展，ceph乘上了OpenStack的春风，进而成为了开源社区受关注较高的项目之一。Ceph可以将多台服务器组成一个超大集群，把这些机器中的磁盘资源整合到一块儿，形成一个大的资源池（PB级别），然后按需分配给应用使用。Ceph分布式存储的优势：
1）CRUSH算法 Crush算法是ceph的两大创新之一，简单来说，ceph摒弃了传统的集中式存储元数据寻址的方案，转而使用CRUSH算法完成数据的寻址操作。 CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。 Crush算法有相当强大的扩展性，理论上支持数千个存储节点。 2）高可用 Ceph中的数据副本数量可以由管理员自行定义，并可以通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性； Ceph可以忍受多种故障场景并自动尝试并行修复； Ceph支持多份强一致性副本，副本能够垮主机、机架、机房、数据中心存放。所以安全可靠。 Ceph存储节点可以自管理、自动修复。无单点故障，容错性强。 3）高性能 因为是多个副本，因此在读写操作时候能够做到高度并行化。理论上，节点越多，整个集群的IOPS和吞吐量越高。 另外一点ceph客户端读写数据直接与存储设备(osd) 交互。在块存储和对象存储中无需元数据服务器。 4）高扩展性 Ceph不同于swift，客户端所有的读写操作都要经过代理节点。一旦集群并发量增大时，代理节点很容易成为单点瓶颈。 Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。 Ceph扩容方便、容量大。能够管理上千台服务器、EB级的容量。 5）特性丰富 Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。 在国内一些公司的云环境中，通常会采用Ceph作为openstack的唯一后端存储来提升数据转发效率。 Ceph统一存储，虽然Ceph底层是一个分布式文件系统，但由于在上层开发了支持对象和块的接口。所以在开源存储软件中，能够一统江湖。至于能不能千秋万代，就不知了。 Ceph提供3种存储方式分别是对象存储，块存储和文件系统，一般我们主要关心的还是块存储，推荐将虚拟机后端存储从SAN过渡到Ceph。Ceph 现在是云计算、虚拟机部署的最火开源存储解决方案，据说有20%的OpenStack部署存储用的都是Ceph的block storage。
2）Ceph分布式存储的基本架构
Ceph的底层是RADOS，RADOS本身也是分布式存储系统，CEPH所有的存储功能都是基于RADOS实现。RADOS采用C++开发，所提供的原生Librados API包括C和C++两种。Ceph的上层应用调用本机上的librados API，再由后者通过socket与RADOS集群中的其他节点通信并完成各种操作。
RADOS向外界暴露了调用接口，即LibRADOS,应用程序只需要调用LibRADOS的接口，就可以操纵Ceph了。这其中，RADOS GW用于对象存储，RBD用于块存储，它们都属于LibRADOS;CephFS是内核态程序，向外界提供了POSIX接口，用户可以通过客户端直接挂载使用。
RADOS GateWay、RBD其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中，RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。这两种方式目前在云计算中应用的比较多。
CEPHFS则提供了POSIX接口，用户可直接通过客户端挂载使用。它是内核态的程序，所以无需调用用户空间的librados库。它通过内核中的net模块来与Rados进行交互。
&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;Ceph之RADOS浅析&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-
RADOS (Reliable, Autonomic Distributed Object Store) 是Ceph的核心之一，作为Ceph分布式文件系统的一个子项目，特别为Ceph的需求设计，能够在动态变化和异质结构的存储设备机群之上提供一种稳定、可扩展、高性能的单一逻辑对象(Object)存储接口和能够实现节点的自适应和自管理的存储系统。
在传统分布式存储架构中，存储节点往往仅作为被动查询对象来使用，随着存储规模的增加，数据一致性的管理会出现很多问题。
而新型的存储架构倾向于将基本的块分配决策和安全保证等操作交给存储节点来做，然后通过提倡客户端和存储节点直接交互来简化数据布局并减小io瓶颈。
RADOS就是这样一个可用于PB级规模数据存储集群的可伸缩的、可靠的对象存储服务。它包含两类节点：存储节点、管理节点。它通过利用存储设备的智能性，将诸如一致性数据访问、冗余存储、错误检测、错误恢复分布到包含了上千存储节点的集群中，而不是仅仅依靠少数管理节点来处理。
RADOS中的存储节点被称为OSD(object storage device)，它可以仅由很普通的组件来构成，只需要包含CPU、网卡、本地缓存和一个磁盘或者RAID，并将传统的块存储方式替换成面向对象的存储。
在PB级的存储规模下，存储系统一定是动态的：系统会随着新设备的部署和旧设备的淘汰而增长或收缩，系统内的设备会持续地崩溃和恢复，大量的数据被创建或者删除。RADOS通过 cluster map来实现这些，cluster map会被复制到集群中的所有部分（存储节点、控制节点，甚至是客户端），并且通过怠惰地传播小增量更新而更新。cluster map中存储了整个集群的数据的分布以及成员。
通过在每个存储节点存储完整的cluster map，存储设备可以表现的半自动化，通过peer-to-peer的方式（比如定义协议）来进行数据备份、更新，错误检测、数据迁移等等操作。这无疑减轻了占少数的monitor cluster（管理节点组成的集群）的负担。
设计如下：
一个RADOS系统包含大量的OSDs 和 很少的用于管理OSD集群成员的monitors。OSD的组成如简介所说。而monitor是一些独立的进程，以及少量的本地存储，monitor之间通过一致性算法保证数据的一致性。
Cluster Map
存储节点集群通过monitor集群操作cluster map来实现成员的管理。cluster map 描述了哪些OSD被包含进存储集群以及所有数据在存储集群中的分布。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/gu-zhang-pai-chu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/gu-zhang-pai-chu/</guid>
      <description>故障排除 1）修改 OSD CRUSH weight 1.1）问题描述
部署完成后，集群处于 PG Degraded 状态，经查 ceph health detail，发现 PG 的 acting OSD 只有 [0]，而不是两个。查 osd tree，osd 日志等，看不出明显问题。
1.2）原因分析
我的 Ceph 集群的 OSD 的 weight 都是 0！！
[root@ceph1]# /etc/ceph# ceph osd tree # id weight type name up/down reweight -1 0 root default -2 0 host ceph1 0 0 osd.0 up 1 2 0 osd.2 up 1 -3 0 host ceph2 1 0 osd.1 up 1 3 0 osd.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/images/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/images/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ji-qun-guan-li/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ji-qun-guan-li/</guid>
      <description>集群管理 Ceph常规操作及常见问题梳理
Ceph集群管理
每次用命令启动、重启、停止Ceph守护进程（或整个集群）时,必须指定至少一个选项和一个命令,还可能要指定守护进程类型或具体例程。
命令格式如
{commandline} [options] [commands] [daemons] 常用的commandline为&amp;quot;ceph&amp;rdquo;,对应的options如下表:
对应的commands如下表:
能指定的daemons(守护进程)类型包括mon,osd及mds。
通过SysVinit机制运行ceph:
在 CentOS、Redhat、发行版上可以通过传统的SysVinit运行Ceph，Debian/Ubuntu的较老的版本也可以用此方法。 使用SysVinit管理Ceph守护进程的语法如下:
[root@ceph ~] sudo /etc/init.d/ceph [options] [start|restart] [daemonType|daemonID] 1）管理Ceph集群内所有类型的守护进程： 通过缺省[daemonType|daemonID],并添加&amp;rdquo;-a&amp;rdquo; options,就可以达到同时对集群内所有类型的守护进程进行启动、关闭、重启等操作目的。
 启动默认集群(ceph)所有守护进程:  [root@ceph ~] sudo /etc/init.d/ceph -a start 停止默认集群(ceph)所有守护进程:
[root@ceph ~] sudo /etc/init.d/ceph -a stop 如果未使用&amp;rdquo;-a&amp;quot;选项,以上命令只会对当前节点内的守护进程生效。
2）管理Ceph集群内指定类型的守护进程： 根据命令语法,要启动当前节点上某一类的守护进程,只需指定对应类型及ID即可。
 启动进程,以OSD进程为例：  #启动当前节点内所有OSD进程 [root@ceph ~] sudo /etc/init.d/ceph start osd #启动当前节点内某一个OSD进程,以osd.0为例 [root@ceph ~] sudo /etc/init.d/ceph start osd.0 重启及关闭进程,以OSD进程为例:
#关闭当前节点内所有OSD进程 [root@ceph ~] sudo /etc/init.d/ceph stop osd #关闭当前节点内某一个OSD进程,以osd.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/readme/</guid>
      <description>Ceph Ceph官方文档：http://docs.ceph.com/docs/master/#
Ceph官网：https://ceph.com
Ceph可视化监控系统：https://ceph.com/planet/快速构建ceph可视化监控系统/
中文社区：http://ceph.org.cn
Red Hat Ceph存储—《深入理解Ceph架构》：http://ceph.org.cn/2018/06/29/red-hat-ceph存储-《深入理解ceph架构》/
中文文档：相较于官网比较滞后，但不影响基础使用：http://docs.ceph.org.cn
相关文档：https://www.cnblogs.com/luohaixian/category/1135307.html</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ri-chang-cao-zuo-ming-ling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ri-chang-cao-zuo-ming-ling/</guid>
      <description>日常操作命令 查看状态命令： 查看ceph集群状态：ceph -s 查看mon状态：ceph mon stat 查看msd状态：ceph msd stat 查看osd状态：ceph osd stat 查看osd目录树（可以查看每个osd挂在哪台机，是否已启动）：ceph osd tree 启动ceph进程命令： 需要在对应的节点进行启动（如果对应节点没有该服务，会进行提示） 启动mon进程：service ceph start mon.ceph-node1 启动msd进程：service ceph start msd.ceoh-node1 启动osd进程：service ceph start osd.0（在ceph-node1上） 启动osd进程：service ceph start osd.1（在ceph-node2上） 启动osd进程：service ceph start osd.2（在ceph-node3上） 查看机器的监控状态 # ceph health 查看ceph的实时运行状态 # ceph -w 检查信息状态信息 # ceph -s 查看ceph存储空间 [root@client ~]# ceph df 删除一个节点的所有的ceph数据包 # ceph-deploy purge ceph-node1 # ceph-deploy purgedata ceph-node1 为ceph创建一个admin用户并为admin用户创建一个密钥，把密钥保存到/etc/ceph目录下： # ceph auth get-or-create client.admin mds &amp;#39;allow&amp;#39; osd &amp;#39;allow &amp;#39; mon &amp;#39;allow &amp;#39; &amp;gt; /etc/ceph/ceph.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ying-jian-xuan-ze/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/ying-jian-xuan-ze/</guid>
      <description>硬件选择 在规划Ceph分布式存储集群环境的时候，对硬件的选择很重要，这关乎整个Ceph集群的性能，下面梳理到一些硬件的选择标准，可供参考：
1）CPU选择 Ceph metadata server会动态的重新分配负载，它是CPU敏感性的，所以Metadata Server应该有比较好的处理器性能 (比如四核CPU). Ceph OSDs运行RADOS服务，需要通过CRUSH来计算数据的存放位置，replicate数据，以及维护Cluster Map的拷贝，因此OSD也需要合适的处理性能 Ceph Monitors 简单的维护了Cluster Map的主干信息所以这个是CPU不敏感的。
2）RAM选择 Metadata servers 以及Monitors 必须能够快速的提供数据，因此必须有充足的内存(e.g., 1GB of RAM per daemon instance). OSDs 在执行正常的操作时不需要过多的内存 (e.g., 500MB of RAM per daemon instance);但是 执行恢复操作时，就需要大量的内存(e.g., ~1GB per 1TB of storage per daemon). Generally, 而且是越多越好。
3）Data Storage选择 规划数据存储时要考虑成本和性能的权衡。同时OS操作，同时多个后台程序对单个驱动器进行读写操作会显着降低性能。也有文件系统的限制考虑：BTRFS对于生产环境来说不是很稳定，但有能力记录journal和并行的写入数据，而XFS和EXT4会好一点。
提示:不推荐单个磁盘的分区上运行多个OSD。不推荐单个磁盘的分区上运行一个OSD和一个监视器或元数据服务。
存储驱动器受寻道时间，访问时间，读取和写入时间，以及总吞吐量的限制。这些物理限制会影响整个系统的性能，尤其是在恢复过程中。我们建议为操作系统和软件使用专用的驱动器，并且为你在主机上运行每个OSD守护分配一个驱动器。大多数“慢OSD”的问题的产生是由于在一个操作系统同一驱动器上运行多个OSDs和/或多个日志。
由于解决性能问题的一小部分的成本可能超过额外的磁盘驱动器的成本，因此你可以加快你的的集群设计规划，为了避免OSD存储驱动器负荷过重。
在每个硬盘驱动器上同时运行多个Ceph的OSD守护程序，但是这可能会导致资源争用，并降低整体吞吐量。你可能把日志和对象数据存储在相同的驱动器上，但这样可能会增加所花费在记录写入操作和发送ACK给客户端的时间。在CEPH可以ACK对于写入操作前,Ceph必须把操作写入到日志。
BTRFS文件系统的日志数据和对象数据的同时可以写，而XFS和ext4的不能。Ceph的推荐做法，分开在单独的驱动器上运行操作系统，OSD数据和OSD日志。
4）固态硬盘选择 性能改进的机会之一是使用固态硬盘（SSD），以减少随机访问时间，读取等待时间，同时吞吐量加速。固态硬盘每GB的费用与硬盘驱动器相比往往超过10倍之多，但固态硬盘往往表现至少比硬盘驱动器快100倍的访问时间。
固态硬盘没有移动机械部件，所以他们不需要受同类型硬盘驱动器的限制。尽管固态硬盘有明显的局限性。重要的是考虑其连续读取和写入的性能。当存储多个OSDs的多个日志时，有400MB/s的顺序写入吞吐量的SSD的性能，相比机械盘120MB/s的顺序写入吞吐量，SSD更好、更快。
固态硬盘的OSD对象存储成本高昂，通过存储一个OSD的日志在一个单独的硬盘驱动器SSD和OSD的对象数据上时，OSDs上可能会看到一个显着的性能提升。OSD日志配置默认在/var/lib/ceph/osd/$cluster-$id/journal里。你可以挂载这个路径到SSD或SSD的分区上，将日志文件和数据文件分别存放在不同的磁盘。
5）Networks选择 建议每台机器最少两个千兆网卡，现在大多数普通硬盘吞的吐量都能达到100MB/s，网卡应该能处理所以OSD硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于public network和cluster_network。集群网络（最好别连接到互联网）用于处理由数据复制产生的额外负载，并且有助于阻止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在OSD数据复制时不能回到active+clean状态。请考虑部署万兆网卡。通过1Gbps网络复制1TB数据耗时3小时，而3TB（一个典型的驱动配置）需要9小时，与之相反，如果使用10Gbps复制时间可分别缩减到20分钟和1小时。
在一个PB级集群中，OSD磁盘失败是常态，而非异常；在性价比合理的前提下，系统管理员想让PG尽快从degraded（降级）状态恢复到active+clean状态。采用10G网卡值得考虑使用。每个网络的顶级机架路由器到核心路由器通信应该有更快的吞吐量，例如，40Gbps~100Gbps。
6）其他注意事项： 可以在每台主机上运行多个OSD进程，但应该确保OSD硬盘的吞吐量总和不超过客户端读取或写入数据所需的网络带宽。也应该考虑在每个主机上数据的存储率。如果一个特定的主机上的百分比较大，它可能会导致问题:为了防止数据丢失,会导致Ceph停止操作。
当每台主机上运行多个OSD进程时，还需要保证内核是最新的。 当每台主机上运行多个OSD进程时（如&amp;gt;20）会产生很多的线程，特别是进行recovery和relalancing操作。许多Linux内核默认线程限最大数量比较小（例如，32k的）。如果您遇到这方面的问题，可以考虑把kernel.pid_max设置的高一点。理论上的最大值为4,194,303。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/yun-wei/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/ceph/yun-wei/</guid>
      <description>运维 一、Ceph简单介绍 1）OSDs:Ceph的OSD守护进程（OSD）存储数据，处理数据复制，恢复，回填，重新调整，并通过检查其它Ceph OSD守护程序作为一个心跳 向Ceph的监视器报告一些检测信息。Ceph的存储集群需要至少2个OSD守护进程来保持一个 active + clean状态.（Ceph默认制作2个备份，但可以调整它）
2）Monitors:Ceph的监控保持集群状态映射，包括OSD(守护进程)映射,分组(PG)映射，和CRUSH映射。 Ceph 保持一个在Ceph监视器, Ceph OSD 守护进程和 PG的每个状态改变的历史（称之为“epoch”）。
3）MDS:MDS是Ceph的元数据服务器，代表存储元数据的Ceph文件系统（即Ceph的块设备和Ceph的对象存储不使用MDS）。Ceph的元数据服务器使用POSIX文件系统，用户可以执行基本命令如 ls, find,等，并且不需要在Ceph的存储集群上造成巨大的负载。
Ceph把客户端的数据以对象的形式存储到了存储池里。利用CRUSH算法，Ceph可以计算出安置组所包含的对象，并能进一步计算出Ceph OSD集合所存储的安置组。CRUSH算法能够使Ceph存储集群拥有动态改变大小、再平衡和数据恢复的能力。
二、Ceph存储特点
 Object：有原生的API，而且也兼容Swift和S3的API Block：支持精简配置、快照、克隆 File：Posix接口，支持快照  Ceph也是分布式存储系统，它的特点是：
 高扩展性：使用普通x86服务器，支持10~1000台服务器，支持TB到PB级的扩展。 高可靠性：没有单点故障，多数据副本，自动管理，自动修复。 高性能：数据分布均衡，并行化度高。对于objects storage和block storage,不需要元数据服务器。  无论你是想使用Ceph对象存储或是以Ceph块设备服务至云平台 ，部署Ceph文件系统或者为了其他目的而使用Ceph，所有的 Ceph存储集群部署都是从设置每个Ceph节点，配置网络和Ceph存储集群开始的。一个Ceph存储集群要求至少有一个Ceph监视器和两个Ceph OSD守护进程。当运行Ceph文件系统客户端时，必须要有Ceph元数据服务器。
Ceph提供了3种使用场景
 分布式文件系统CephFS。多个客户端mount CephFS到本地，CephFS遵循POSIX接口，使用体验类似于ext4等本地文件系统。类似于其他分布式文件系统，各个CephFS客户端共享同一命名空间。 RadosGW（rgw）对象存储。rgw使用场景类似于Amazon S3，据个人理解也类似于七牛云存储。 块设备rbd（Rados Block Device）。Ceph提供虚拟的rbd块设备，用户像使用SATA盘那样的物理块设备一样使用rbd。rbd的使用是排他的，每个rbd块设备是用户私有的，相对的，CephFS的使用方式是共享的。虚拟化和云计算的发展正当盛年，IaaS结合rbd块设备这一使用方式有如干柴遇烈火，因此rbd是Ceph社区开发的重心之一。本文也主要从rbd的视角来了解Ceph。  </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/diao-du/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/diao-du/</guid>
      <description>调度 常见的分布式文件系统有，GFS、HDFS、Lustre 、Ceph 、GridFS 、mogileFS、TFS、FastDFS等。各自适用于不同的领域。它们都不是系统级的分布式文件系统，而是应用级的分布式文件存储服务。
Google学术论文，这是众多分布式文件系统的起源
Google File System（大规模分散文件系统）
MapReduce （大规模分散FrameWork）
BigTable（大规模分散数据库）
Chubby（分散锁服务）
搜索Google三大论文中文版(Bigtable、 GFS、 Google MapReduce)。
原版地址链接：
http://labs.google.com/papers/gfs.html http://labs.google.com/papers/bigtable.html http://labs.google.com/papers/mapreduce.html
GFS（Google File System） Google公司为了满足本公司需求而开发的基于Linux的专有分布式文件系统。。尽管Google公布了该系统的一些技术细节，但Google并没有将该系统的软件部分作为开源软件发布。
下面分布式文件系统都是类 GFS的产品。
HDFS Hadoop 实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。 Hadoop是Apache Lucene创始人Doug Cutting开发的使用广泛的文本搜索库。它起源于Apache Nutch，后者是一个开源的网络搜索引擎，本身也是Luene项目的一部分。Aapche Hadoop架构是MapReduce算法的一种开源应用，是Google开创其帝国的重要基石。
Ceph 是加州大学圣克鲁兹分校的Sage weil攻读博士时开发的分布式文件系统。并使用Ceph完成了他的论文。
说 ceph 性能最高，C++编写的代码，支持Fuse，并且没有单点故障依赖， 于是下载安装， 由于 ceph 使用 btrfs 文件系统， 而btrfs 文件系统需要 Linux 2.6.34 以上的内核才支持。
可是ceph太不成熟了，它基于的btrfs本身就不成熟，它的官方网站上也明确指出不要把ceph用在生产环境中。
Lustre Lustre是一个大规模的、安全可靠的，具备高可用性的集群文件系统，它是由SUN公司开发和维护的。
该项目主要的目的就是开发下一代的集群文件系统，可以支持超过10000个节点，数以PB的数据量存储系统。
目前Lustre已经运用在一些领域，例如HP SFS产品等。
适合存储小文件、图片的分布文件系统研究 FastDFS分布文件系统 （我写的）
TFS（Taobao File System）安装方法 （我写的）
用于图片等小文件大规模存储的分布式文件系统调研
架构高性能海量图片服务器的技术要素</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/drbd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/drbd/</guid>
      <description>DRBD https://www.cnblogs.com/kevingrace/category/925329.html</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/bu-shu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/bu-shu/</guid>
      <description>部署 1）服务器信息（centos6.9）
描述 ip地址 主机名称 分组 跟踪服务器1 192.168.10.200 Fastdfs_tracker_t1 跟踪服务器2 192.168.10.201 Fastdfs_tracker_t2 存储服务器1 192.168.10.202 Fastdfs_storage_s1 group1 存储服务器2 192.168.10.203 Fastdfs_storage_s2 group2 存储服务器3 192.168.10.204 Fastdfs_storage_s3 group3 客户机1 192.168.10.205 Fastdfs_client 2）FastDFS安装（所有tracker服务器、storage服务器、客户端机器都要有如下操作。这里以Fastdfs_tracker_t1服务器操作为例
1）编译和安装所需的依赖包 [root@Fastdfs_tracker_t1 ~]# yum install make cmake gcc gcc-c++ 2）安装libfastcommon [root@Fastdfs_tracker_t1 ~]# cd /usr/local/src/ [root@Fastdfs_tracker_t1 src]# wget https://codeload.github.com/happyfish100/libfastcommon/tar.gz/V1.0.7 -O libfastcommon-1.0.7.tar.gz [root@Fastdfs_tracker_t1 src]# tar zxf libfastcommon-1.0.7.tar.gz [root@Fastdfs_tracker_t1 src]# cd libfastcommon-1.0.7 [root@Fastdfs_tracker_t1 libfastcommon-1.0.7]# ./make.sh [root@Fastdfs_tracker_t1 libfastcommon-1.0.7]# ./make.sh install libfastcommon 默认安装到了/usr/lib64/libfastcommon.so和/usr/lib64/libfdfsclient.so 因为FastDFS主程序设置的lib目录是/usr/local/lib，所以需要创建软链接 [root@Fastdfs_tracker_t1 libfastcommon-1.0.7]# ln -s /usr/lib64/libfastcommon.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/chang-yong-ming-ling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/chang-yong-ming-ling/</guid>
      <description>常用命令 1）上传文件命令：fdfs_upload_file 不仅可以上传图片文件，也可以上传其他文件。 [root@Fastdfs_client src]# echo &amp;#34;hahaha&amp;#34; &amp;gt; test.txt [root@Fastdfs_client src]# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/test.txt group3/M00/00/00/wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt 上传成功后，返回一串带有组和路径标识的字符串，这是文件的id，然后到group3组内的存储服务器上查看这个文件的id信息 [root@Fastdfs_storage_s3 ~]# ll /fastdfs/storage/data/00/00/ total 84 -rw-r--r--. 1 root root 7 Feb 27 11:02 wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt [root@Fastdfs_storage_s3 ~]# cat /fastdfs/storage/data/00/00/wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt hahaha [root@Fastdfs_storage_s3 ~]# md5sum /fastdfs/storage/data/00/00/wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt 5a6d311c0d8f6d1dd03c1c129061d3b1 /fastdfs/storage/data/00/00/wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt 2）下载文件命令：fdfs_download_file 在客户端机器上将上面上传到Fastdfs存储里文文件下载下来 [root@Fastdfs_client ~]# cd /mnt/ [root@Fastdfs_client mnt]# /usr/bin/fdfs_download_file /etc/fdfs/client.conf group3/M00/00/00/wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt [root@Fastdfs_client mnt]# ls wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt [root@Fastdfs_client mnt]# cat wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt hahaha [root@Fastdfs_client mnt]# md5sum wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt 5a6d311c0d8f6d1dd03c1c129061d3b1 wKgKzFqUyiqAabn8AAAAB-kMomQ229.txt 下载后可以对比文件的md5值。 3）查看文件信息命令：fdfs_file_info [root@Fastdfs_client ~]# /usr/bin/fdfs_file_info /etc/fdfs/client.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/gai-shu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/gai-shu/</guid>
      <description>概述 二、FastDFS分布式系统架构介绍 FastDFS：是一个开源的轻量级分布式文件系统，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合中小文件（建议范围：4KB &amp;lt; file_size &amp;lt;500MB），对以文件为载体的在线服务，如相册网站、视频网站等。
FastDFS是为互联网应用量身定做的分布式文件系统，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标。和现有的类Google FS分布式文件系统相比，FastDFS的架构和设计理念有其独到之处，主要体现在轻量级、分组方式和对等结构三个方面。
FastDFS架构图 FastDFS服务端有两个角色：跟踪器（tracker）和存储节点（storage）。跟踪器主要做调度工作，在访问上起负载均衡的作用。
FastDFS模块介绍 1）tracker server：跟踪服务器，用来调度来自客户端的请求，且在内存中记录所有存储组和存储服务器的信息状态。
2）storage server：存储服务器，用来存储文件(data)和文件属性(metadata)。
3）client：客户端，业务请求发起方，通过专用接口基于TCP协议与tracker server和storage server进行交互。
4）group：组，也可称为卷，同组内上的文件是完全相同的。
5）文件标识：包括两部分，组名（group）和文件名（含路径）
6）文件相关属性：键值对(Key Value Pair)方式
7）文件名：与原文件名并不相同。由storage server根据特定信息生成，并且可逆，文件名包含：源存储服务器的IP地址、文件创建时间戳、文件大小、随机数和文件扩展名等。
FastDFS由跟踪服务器(Tracker Server)、存储服务器(Storage Server)和客户端(Client)构成，其中：
跟踪服务器Tracker Server 主要做调度工作，起到均衡的作用；负责管理所有的 storage server和 group，每个 storage 在启动后会连接 Tracker，告知自己所属 group 等信息，并保持周期性心跳。tracker根据storage的心跳信息，建立group==&amp;gt;[storage serverlist]的映射表。
Tracker需要管理的元信息很少，会全部存储在内存中；另外tracker上的元信息都是由storage汇报的信息生成的，本身不需要持久化任何数据，这样使得tracker非常容易扩展，直接增加tracker机器即可扩展为tracker cluster来服务，cluster里每个tracker之间是完全对等的，所有的tracker都接受stroage的心跳信息，生成元数据信息来提供读写服务。
存储服务器Storage Server 主要提供容量和备份服务；以 group 为单位，每个 group 内可以有多台 storage server，数据互为备份。以group为单位组织存储能方便的进行应用隔离、负载均衡、副本数定制（group内storage server数量即为该group的副本数），比如将不同应用数据存到不同的group就能隔离应用数据，同时还可根据应用的访问特性来将应用分配到不同的group来做负载均衡；缺点是group的容量受单机存储容量的限制，同时当group内有机器坏掉时，数据恢复只能依赖group内地其他机器，使得恢复时间会很长。
group内每个storage的存储依赖于本地文件系统，storage可配置多个数据存储目录，比如有10块磁盘，分别挂载在/data/disk1-/data/disk10，则可将这10个目录都配置为storage的数据存储目录。storage接受到写文件请求时，会根据配置好的规则选择其中一个存储目录来存储文件。为了避免单个目录下的文件数太多，在storage第一次启动时，会在每个数据存储目录里创建2级子目录，每级256个，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据作为本地文件存储到该目录中。
客户端Client 主要是上传下载数据的服务器，也就是我们自己的项目所部署在的服务器。每个客户端服务器都需要安装Nginx
基本架构如下图所示。
FastDFS的存储策略 为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。
在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。
FastDFS的上传过程 FastDFS向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。
Storage Server会定期的向Tracker Server发送自己的存储信息。当Tracker Server Cluster中的Tracker Server不止一个时，各个Tracker之间的关系是对等的，所以客户端上传时可以选择任意一个Tracker。
当Tracker收到客户端上传文件的请求时，会为该文件分配一个可以存储文件的group，当选定了group后就要决定给客户端分配group中的哪一个storage server。当分配好storage server后，客户端向storage发送写文件请求，storage将会为文件分配一个数据存储目录。然后为文件分配一个fileid，最后根据以上的信息生成文件名存储文件。文件名的格式如下：</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/images/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/images/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fastdfs/readme/</guid>
      <description>FastDFS </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fen-bu-shi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/fen-bu-shi/</guid>
      <description>分布式 认为对象存储和文件系统最大的区别就是API，文件系统提供了完整POSIX语义，往往具有层次化的目录结构，对文件可以进行精细的操作(open, read, write, seek, delete等)，相对会复杂一些。相比之下，对象存储就简单的多，对象本义是**B**inary **L**arge **OB**ject(BLOB), 是一个大的二进制文件，是作为一个整体出现，不能对其进行修改，因此对象存储系统有一个显著的特点就是Immutable，即不变性，正是因为这个特点对象存储系统一般只提供put, get, 和delete的操作，并且都是以key/val的形式，val一般是整个blob. 具体到实际的资源，主要指的是像图片，视频，文档，源码，二进制程序等这样的文件，这些文件往往都很小，一般在100k左右，视频可能会大一些，能达到几十兆甚至好几个G。
系统整体对比
   对比说明/文件系统 TFS FastDFS MogileFS MooseFS GlusterFS Ceph     开发语言 C++ C Perl C C C++   开源协议 GPL V2 GPL V3 GPL GPL V3 GPL V3 LGPL   数据存储方式 块 文件/Trunk 文件 块 文件/块 对象/文件/块   集群节点通信协议 私有协议（TCP） 私有协议（TCP） HTTP 私有协议（TCP） 私有协议（TCP）/ RDAM(远程直接访问内存) 私有协议（TCP）   专用元数据存储点 占用NS 无 占用DB 占用MFS 无 占用MDS   在线扩容 支持 支持 支持 支持 支持 支持   冗余备份 支持 支持 - 支持 支持 支持   单点故障 存在 不存在 存在 存在 不存在 存在   跨集群同步 支持 部分支持 - - 支持 不适用   易用性 安装复杂，官方文档少 安装简单，社区相对活跃 - 安装简单，官方文档多 安装简单，官方文档专业化 安装简单，官方文档专业化   适用场景 跨集群的小文件 单集群的中小文件 - 单集群的大中文件 跨集群云存储 单集群的大中小文件    开源协议说明</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/</guid>
      <description>GlusterFS分布式系统维护管理手册
1）管理说明
在解释系统管理时会提供实例，首先提供一个环境说明。 系统节点： IP 别名 Brick 192.168.2.100 server0 /mnt/sdb1 /mnt/sdc1 /mnt/sdd1 192.168.2.101 server1 /mnt/sdb1 /mnt/sdc1 /mnt/sdd1 192.168.2.102 server2 /mnt/sdb1 /mnt/sdc1 /mnt/sdd1 创建了三个节点，并每台虚拟机 mount 三块磁盘作为 Brick 使用，每个 brick 分配了 30G 的虚拟容量。 实例约定 AFR 卷名： afr_vol DHT 卷名： dht_vol Stripe 卷名： str_vol 客户端挂载点： /mnt/gluster 2）系统部署
2.1) 在每个节点上启动glusterd服务 [root@localhost ~]# service glusterd start 2.2) 添加节点到存储池，在其中一个节点上操作 ，如 server0 [root@localhost ~]# gluster peer probe server1 [root@localhost ~]# gluster peer probe server2 //可以使用 gluster peer status 查看当前有多少个节点，显示不包括该节点 2.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/admin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/admin/</guid>
      <description>Admin 接着上一篇Centos7下GlusterFS分布式存储集群环境部署记录文档，继续做一些补充记录，希望能加深对GlusterFS存储操作的理解和熟悉度。
清理glusterfs存储环境 由上面可知，该glusterfs存储集群有四个节点： [root@GlusterFS-master ~]# cat /etc/hosts ....... 192.168.10.239 GlusterFS-master 192.168.10.212 GlusterFS-slave 192.168.10.204 GlusterFS-slave2 192.168.10.220 GlusterFS-slave3 现将四个节点的存储目录/opt/gluster/data全部删除 [root@GlusterFS-master ~]# rm -rf /opt/gluster [root@GlusterFS-slave ~]# rm -rf /opt/gluster [root@GlusterFS-slave2 ~]# rm -rf /opt/gluster [root@GlusterFS-slave3 ~]# rm -rf /opt/gluster [root@GlusterFS-master ~]# gluster volume list models [root@GlusterFS-master ~]# gluster volume info Volume Name: models Type: Distributed-Replicate Volume ID: f1945b0b-67d6-4202-9198-639244ab0a6a Status: Stopped Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: 192.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/admin2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/admin2/</guid>
      <description>Admin2 Gluster设置允许可信任客户端IP 设置只允许192.168.1.*的访问 [root@GlusterFS-master ~]# gluster volume set gluster_share auth.allow 192.168.1.* volume set: success 查看卷信息 [root@GlusterFS-master ~]# gluster volume info Volume Name: gluster_share Type: Distributed-Replicate Volume ID: a9f989bd-7edd-4089-836a-d9f742b8d37a Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: 192.168.10.239:/data/gluster Brick2: 192.168.10.212:/data/gluster Brick3: 192.168.10.204:/data/gluster Brick4: 192.168.10.220:/data/gluster Options Reconfigured: auth.allow: 192.168.1.* 注意上面最后一行卷信息，说明只允许客户端ip为192.168.10.*网段的机器挂载。 然后在192.168.10.213客户端进行挂载，发现就挂载不上了 [root@Client ~]# mount -t glusterfs 192.168.10.239:gluster_share /opt/gfsmount/ [root@Client ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 38G 4.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/geng-huan-gu-zhang-brick/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/geng-huan-gu-zhang-brick/</guid>
      <description>更换故障Brick 前面已经介绍了GlusterFS分布式存储集群环境部署记录，现在模拟下更换故障Brick的操作：
1）GlusterFS集群系统一共有4个节点，集群信息如下：
分别在各个节点上配置hosts、同步好系统时间，关闭防火墙和selinux [root@GlusterFS-slave data]# cat /etc/hosts 192.168.10.239 GlusterFS-master 192.168.10.212 GlusterFS-slave 192.168.10.204 GlusterFS-slave2 192.168.10.220 GlusterFS-slave3 ------------------------------------------------------------------------------------ 分别在四个节点机上使用df创建一个虚拟分区，然后在这个分区上创建存储目录 [root@GlusterFS-master ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 36G 1.8G 34G 5% / devtmpfs 2.9G 0 2.9G 0% /dev tmpfs 2.9G 0 2.9G 0% /dev/shm tmpfs 2.9G 8.5M 2.9G 1% /run tmpfs 2.9G 0 2.9G 0% /sys/fs/cgroup /dev/vda1 1014M 143M 872M 15% /boot /dev/mapper/centos-home 18G 33M 18G 1% /home tmpfs 581M 0 581M 0% /run/user/0 dd命令创建一个虚拟分区出来，格式化并挂载到/data目录下 [root@GlusterFS-master ~]# dd if=/dev/vda1 of=/dev/vdb1 2097152+0 records in 2097152+0 records out 1073741824 bytes (1.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/administration-guide/readme/</guid>
      <description>Administration Guide </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/gai-shu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/gai-shu/</guid>
      <description>概述 GlusterFS系统是一个可扩展的网络文件系统，相比其他分布式文件系统，GlusterFS具有高扩展性、高可用性、高性能、可横向扩展等特点，并且其没有元数据服务器的设计，让整个服务没有单点故障的隐患。Glusterfs是一个横向扩展的分布式文件系统，就是把多台异构的存储服务器的存储空间整合起来给用户提供统一的命名空间。用户访问存储资源的方式有很多，可以通过NFS，SMB，HTTP协议等访问，还可以通过gluster本身提供的客户端访问。
GlusterFS是Scale-Out存储解决方案Gluster的核心，它是一个开源的分布式文件系统，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。
**GlusterFS 适合大文件还是小文件存储？**弹性哈希算法和Stripe 数据分布策略，移除了元数据依赖，优化了数据分布，提高数据访问并行性，能够大幅提高大文件存储的性能。对于小文件，无元数据服务设计解决了元数据的问题。但GlusterFS 并没有在I/O 方面作优化，在存储服务器底层文件系统上仍然是大量小文件，本地文件系统元数据访问是一个瓶颈，数据分布和并行性也无法充分发挥作用。因此，GlusterFS 适合存储大文件，小文件性能较差，还存在很大优化空间。
GlusterFS 在企业中应用场景 理论和实践上分析，GlusterFS目前主要适用大文件存储场景，对于小文件尤其是海量小文件，存储效率和访问性能都表现不佳。海量小文件LOSF问题是工业界和学术界公认的难题，GlusterFS作为通用的分布式文件系统，并没有对小文件作额外的优化措施，性能不好也是可以理解的。
Media − 文档、图片、音频、视频
Shared storage − 云存储、虚拟化存储、HPC（高性能计算）
Big data − 日志文件、RFID（射频识别）数据
1）GlusterFS存储的几个术语 Brick：GlusterFS中的存储单元，通过是一个受信存储池中的服务器的一个导出目录。可以通过主机名和目录名来标识，如&#39;SERVER:EXPORT&amp;rsquo;。
Client：挂载了GlusterFS卷的设备。
GFID：GlusterFS卷中的每个文件或目录都有一个唯一的128位的数据相关联，其用于模拟inode
Namespace：每个Gluster卷都导出单个ns作为POSIX的挂载点。
Node：一个拥有若干brick的设备。
RDMA：远程直接内存访问，支持不通过双方的OS进行直接内存访问。
RRDNS：round robin DNS是一种通过DNS轮转返回不同的设备以进行负载均衡的方法
Self-heal：用于后台运行检测复本卷中文件和目录的不一致性并解决这些不一致。
Split-brain：脑裂
Volfile：Glusterfs进程的配置文件，通常位于/var/lib/glusterd/vols/volname
Volume：一组bricks的逻辑集合
a）Trusted Storage Pool
• 一堆存储节点的集合
• 通过一个节点“邀请”其他节点创建，这里叫probe
• 成员可以动态加入，动态删除
添加命令如下： node1# gluster peer probe node2
删除命令如下：
node1# gluster peer detach node3
2）Bricks
• Brick是一个节点和一个导出目录的集合，e.g. node1:/brick1
• Brick是底层的RAID或磁盘经XFS或ext4文件系统格式化而来，所以继承了文件系统的限制
• 每个节点上的brick数是不限的
• 理想的状况是，一个集群的所有Brick大小都一样。
3）Volumes</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/images/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/images/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/install-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/install-guide/</guid>
      <description>Install Guide 之前已经简单地对
GlusterFS分布式文件系统
做了介绍，下面就该环境部署做一记录：
环境说明 GlusterFS至少需要两台服务器搭建，服务器配置最好相同，每个服务器两块磁盘，一块是用于安装系统，一块是用于GlusterFS。 192.168.10.10 GlusterFS-master（主节点） Centos7.4 192.168.10.20 GlusterFS-slave （从节点） Centos7.4 192.168.10.30 Client （客户端） 环境准备 由于GlusterFS需要使用网络，因此还必须事先根据环境设置防火墙规则，关闭SELinux。
这里我将上面三台服务器的防火墙和Selinux全部关闭
[root@GlusterFS-master ~]# setenforce 0 [root@GlusterFS-master ~]# getenforce [root@GlusterFS-master ~]# sed -i &amp;#39;s_SELINUX=enforcing_SELINUX=disabled_g&amp;#39; /etc/sysconfig/selinux [root@GlusterFS-master ~]# cat /etc/sysconfig/selinux |grep &amp;#34;SELINUX=disabled&amp;#34; SELINUX=disabled [root@GlusterFS-master ~]# systemctl stop firewalld [root@GlusterFS-master ~]# systemctl disable firewalld [root@GlusterFS-master ~]# firewall-cmd --state not running 由于GlusterFS并没有服务器与元数据等概念，因此所有服务器的设置都相同。首先要做主机名的设置（如果操作时都用ip地址，不使用主机名，那么就不需要做hosts绑定）： [root@GlusterFS-master ~]# hostnamectl --static set-hostname GlusterFS-master [root@GlusterFS-master ~]# cat /etc/hostname GlusterFS-master [root@GlusterFS-master ~]# vim /etc/hosts .</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/readme/</guid>
      <description>GlusterFS Gluster官网：https://www.gluster.org
官方文档：https://docs.gluster.org/en/latest/
相关资料：https://www.cnblogs.com/wangtao1993/p/6030918.html</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/shu-ju-de-hui-fu-ji-523628-afr-de-shuo-ming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/glusterfs/shu-ju-de-hui-fu-ji-523628-afr-de-shuo-ming/</guid>
      <description>数据的恢复机制(AFR)的说明 GlusterFSFS恢复数据都是基于副本卷来说的，GlusterFSFS复制卷是采用镜像的方式做的，并且是同步事务性操作。简单来说就是，某一个客户要写文件时，先把这个文件锁住，然后同时写两个或多个副本，写完后解锁，这个操作才算结束。那么在写某一个副本时发生故障没有写成功，或者运行过程中某一个节点断电了，造成数据丢失了，等等，就能通过另一个副本来恢复。
现在这里说一个疑问： 就是GlusterFS写副本时同步写的，就是客户端同时写两份数据，这样就会产生两倍的流量，测2副本的分布式复制卷性能时，能明确看到性能只有无副本的一半，或者只有读的一半；另一个分布式文件系统ceph就不是这样，是异步来写副本的，就是写到一个主OSD（ceph的存储单元）就返回了，这个OSD再通过内部网络异步写到其他的OSD，这样不是更快了。那么这两种方法有什么优缺点呢，那种比较好，或者各自为什么采用这样的方法？
说道恢复就有什么时候恢复，怎么恢复，凭什么说这个副本是好的，那个副本是坏的呢，这样的问题，一个一个来说吧。
**1）首先，什么时候恢复？**有这样三种场景会触发恢复，宕机的节点恢复正常时；副本缺失的文件被读写到时，比如运行如下命令：
ls -l &amp;lt;file-path-on-gluster-mount&amp;gt;； 每十分钟gluster会自行检查；手动下命令触发恢复，命令为gluster volume heal VOLNAME **2）怎么恢复？**在这三种环境的任何一种下，gluster都会做检查，看需不需要来个恢复，检查什么呢，就是changelog，通过这个changelog来决定哪个副本坏了，要修复了。
**3）凭什么说它坏了呢？**刚才说了changelog会记录的，记录的什么呢，就是这个文件操作了什么，这个可以从文件的扩展属性反正出来，每一个文件都有一个扩展属性，主要记录了这个文件操作了什么，以及所有其他的副本操作了什么，副本的扩展属性如果不一样，那么就是有问题，要恢复自己，还是凭自己去恢复其他副本，都看这个扩展属性了，可以用命令getfattr -m . -d -e hex &amp;lt;FILENAME&amp;gt;，（和getfattr对应，有一个setfattr命令是可以设置这些属性的，具体命令为setfattr -n trusted.glusterfs.volume-id -v 0x937d9caf46544ed0a2d22e25edb23a75 /brick2，）这样设置了/brick2所在卷的id，查到扩展属性的值就是如下图所示的这样的一个东西：
这张图中值得注意的是，trusted.afr.repvol1-client-0，还有trusted.afr.repvol1-client-1，这两条就是自己的和副本的扩展属性了。先讲名字，repvol1是卷名，client是固定的，0或1是subvolume-index，是brick的一个编号。后面的值看起来一大串，一共有24bit，分三部分，每部分4byte，如下图所示：
分别表示数据，元数据，和entry，数据就是文件内容啦，元数据就是属性这些，entry我不知道翻译成什么好，就是gfid，那么这三个东西每一个变化了在这个扩展属性上都会做相应的变化，怎么变化呢，这三个部分分别是三个计数，操作文件之前要先写计数，简单来说可以理解为加一，操作完就减一，这样最后还是保持0，就表示OK。扩展属性被设置的文件和目录会在/
&amp;lt;BRICK&amp;gt;/.glusterfs/indices/xattrop目录中有一个索引，具体如下所示，这个文件的内容好像是会定时清空的，啥时候清空呢？
每一个文件不仅记了自己的状态，还记了所有副本的状态，根据这些状态的组合，有下面几种情况：
IGNORANT：压根没有changelog，比如说这个文件副本已经丢失了，这样的情况changelog也跟着丢失了
INNOCENT ：表示自己和其他副本的计数值都是0，表示双方都OK的
FOOL：表示自己的计数不为0，就是说加了没有减，这之间操作出现问题了，而其他副本为0，就是自己有问题别人没问题，让别人来恢复我
WISE：相反，自己是0，别人不是0 ，自己没问题别人有问题，自己来恢复别人。
涉及到数据恢复，有如下几种场景： 1）所有文件都是IGNORANT，这是手动触发了heal，也就是通过命令，这是怎么恢复呢，就找UID最小的文件作为源，去恢复大小为0的那些文件。
2）有一个节点为WISE，其他事FOOL，或其他非WISE的状态，那么就以WISE去恢复其他节点。
3）好几个都是WISE，就是好几个副本都说自己正常，同时还说别人不正常，这就是脑裂现象，这样就必须靠管理员手动找出脑裂的副本，自行判断哪些是对的哪些不对，自行恢复了，通常的做法留下一个对的副本，其他都删除，同样还要删除 /&amp;lt;BRICK&amp;gt;/.glusters这个目录下对应的文件，这样就只有一个WISE副本了，再出发heal，就以这个为源恢复所有副本了，触发命令为 gluster volume heal &amp;lt;VOLMUENAME&amp;gt; full，这里的full是一种自愈方式，全部恢复文件，另一种自愈方式叫diff，是差异化恢复。
通常脑裂的文件时读不出来的，读写它时会报Input/Output error，查看日志/var/log/glusterfs/glustershd.log你会有收获：
要找到脑裂文件，还有一个命令可以用，gluster volume heal
&amp;lt;VOLMUENAME&amp;gt; info split-brain，它的输出如下：
正常情况下，所有brick的entries都是0，这里同一个副本一个是1一个是0，就是不对劲了，通过此方法找到脑裂的文件，再按上面的方法删除也可以。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/mgfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/mgfs/</guid>
      <description>mgfs </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/moosefs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/moosefs/</guid>
      <description>MooseFS https://www.cnblogs.com/kevingrace/category/924882.html</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/readme/</guid>
      <description>分布式文件系统 https://blog.csdn.net/wendowswd/article/details/78319323
分布式文件系统
分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源并不直接与本地节点相连，而是分布于计算网络中的一个或者多个节点的计算机上。目前意义上的分布式文件系统大多都是由多个节点计算机构成，结构上是典型的客户机/服务器模式。流行的模式是当客户机需要存储数据时，服务器指引其将数据分散的存储到多个存储节点上，以提供更快的速度，更大的容量及更好的冗余特性。
分布式文件系统的产生 计算机通过文件系统管理、存储数据，而现在数据信息爆炸的时代中人们可以获取的数据成指数倍的增长，单纯通过增加硬盘个数来扩展计算机文件系统的存储容量的方式，已经不能满足目前的需求。
分布式文件系统可以有效解决数据的存储和管理难题，将固定于某个地点的某个文件系统，扩展到任意多个地点/多个文件系统，众多的节点组成一个文件系统网络。每个节点可以分布在不同的地点，通过网络进行节点间的通信和数据传输。人们在使用分布式文件系统时，无需关心数据是存储在哪个节点上、或者是从哪个节点从获取的，只需要像使用本地文件系统一样管理和存储文件系统中的数据。 .典型代表NFS NFS（Network File System）即网络文件系统，它允许网络中的计算机之间通过TCP/IP网络共享资源。在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。
.NFS的优点如下： 1）节约使用的磁盘空间
客户端经常使用的数据可以集中存放在一台机器上,并使用NFS发布,那么网络内部所有计算机可以通过网络访问,不必单独存储。
2）节约硬件资源
NFS还可以共享软驱,CDROM和ZIP等的存储设备,减少整个网络上的可移动设备的数量。
3）用户主目录设定
对于特殊用户,如管理员等,为了管理的需要,可能会经常登录到网络中所有的计算机,若每个客户端,均保存这个用户的主目录很繁琐,而且不能保证数据的一致性.实际上,经过NFS服务的设定,然后在客户端指定这个用户的主目录位置,并自动挂载,就可以在任何计算机上使用用户主目录的文件。
.NFS面临的问题 1）存储空间不足，需要更大容量的存储。
2）直接用NFS挂载存储，有一定风险，存在单点故障。
3）某些场景不能满足要求，大量的访问磁盘IO是瓶颈。
目前流行的分布式文件系统有许多，如MooseFS、FastDFS、GlusterFS、Ceph、MogileFS等，常见的分布式存储对比如下：
  FastDFS
：一个开源的轻量级分布式文件系统，是纯C语言开发的。它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。FastDFS 针对大量小文件存储有优势。
  GlusterFS
：主要应用在集群系统中，具有很好的可扩展性。软件的结构设计良好，易于扩展和配置，通过各个模块的灵活搭配以得到针对性的解决方案。GlusterFS适合大文件，小文件性能相对较差。
  MooseFS
：比较接近GoogleFS的c++实现，通过fuse支持了标准的posix，支持FUSE，相对比较轻量级，对master服务器有单点依赖，用perl编写，算是通用的文件系统，可惜社区不是太活跃，性能相对其他几个来说较差，国内用的人比较多。
  Ceph
：C++编写，性能很高，支持Fuse，并且没有单点故障依赖；Ceph 是一种全新的存储方法，对应于 Swift 对象存储。在对象存储中，应用程序不会写入文件系统，而是使用存储中的直接 API 访问写入存储。因此，应用程序能够绕过操作系统的功能和限制。在openstack社区比较火，做虚机块存储用的很多！
  GoogleFS
：性能十分好，可扩展性强，可靠性强。用于大型的、分布式的、对大数据进行访问的应用。运用在廉价的硬件上。
  分布式文件系统：Distributed file system, DFS，又叫做网络文件系统：Network File System。一种允许文件通过网络在多台主机上分享的文件系统，可让多机器上的多用户分享文件和存储空间。 特点：在一个分享的磁盘文件系统中，所有节点对数据存储区块都有相同的访问权，在这样的系统中，访问权限就必须由客户端程序来控制。分布式文件系统可能包含的功能有：透通的数据复制与容错。 分布式文件系统是被设计用在局域网。而分布式数据存储，则是泛指应用分布式运算技术的文件和数据库等提供数据存储服务的系统。 决定因素：数据的存储方式、数据的读取速率、数据的安全机制。 发展历史：大致分为三个发展阶段，网络文件系统(1980s)、共享SAN文件系统(1990s)、面向对象的并行文件系统(2000s)。 </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/shi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/distributed-fs/shi/</guid>
      <description>诗 一、各种分布式文件系统对比
1.1 表格对比
   技术 优点 缺点 总结     HDFS 1.大数据批量读写，吞吐量高;2.一次写入，多次读取，顺序读写； 1、交互式应用，低延迟很难满足； 2、不支持多用户并发写相同文件。 如果是很多小文件，nameNode压力大   googleFs 1、成本低，运行在廉价的普通硬件上 、不开源 不开源，使用困难   Tfs 1、 开源 1、小于1M的文件 2、TFS内部是没有任何数据的内存缓冲的 适合单个文件比较小的系统   Lustre 1、 开源 2、 支持POSIX 3、 文件被分割成若干的Chunk，每个chunk是一般为1MB－4MB     Ceph 1、支持POSIX 2、开源  1、 在Linux主流内核中找到ceph 2、不成熟，处于测试推广阶段   MogileFs 1、开源  比FastDFS 差   FastDFS 1、 开源 2、 适合以文件为载体的在线服务 3、 FastDFS没有对文件做分块存储 4、 不需要二次开发即可直接使用 5、 比mogileFS更易维护和使用 6、 直接使用socket通信方式，相对于MogileFS的HTTP方式，效率更高。 1、文件访问方式使用专有API，不支持POSIX    swiftfs   1、基于HDFS   NFS 1、用户和程序可以象访问本地文件一样访问远端系统上的文件      开源的分布式文件/对象系统比较有名的包括Lustre（HPC）GlusterFS（NAS NFS）、HDFS（hadoop）、ceph（虚机块存储）、swift（restful对象存储），各有不同的领域。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/das/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/das/</guid>
      <description>DAS </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/fc-san/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/fc-san/</guid>
      <description>FC-SAN </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/ip-san/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/ip-san/</guid>
      <description>IP-SAN Linux下IP SAN共享存储操作记录 操作需求：
公司之前在阿里云上购买了6台机器，磁盘空间大小不一致，后续IDC建设好后，又将业务从阿里云上迁移到IDC机器上了。
为了不浪费阿里云上的这几台机器资源，打算将这其中的5台机器做成IP SAN共享存储，另一台机器共享这5台的SAN存储，然后跟自己的磁盘一起做成LVM逻辑卷，最后统一作为备份磁盘使用！
1）服务器信息如下：
ip地址 数据盘空间 主机名 系统版本 192.168.10.17 200G ipsan-node01 centos7.3 192.168.10.18 500G ipsan-node02 centos7.3 192.168.10.5 500G ipsan-node03 centos7.3 192.168.10.6 200G ipsan-node04 centos7.3 192.168.10.20 100G ipsan-node05 centos7.3 192.168.10.10 100G ipsan-node06 centos7.3 前5个node节点作为IP-SAN存储的服务端，第6个node节点作为客户端，用来共享前5个节点的IP-SAN存储，然后第6个node节点利用这5个共享过来的IP-SAN存储和 自己的100G存储做lvm逻辑卷，最终组成一个大的存储池来使用！ 首先将这6个node节点机对应的盘做格式化(6台机器的数据盘都是挂载到/data下的，需要先卸载/data，然后格式化磁盘) 接着关闭各节点服务器的iptables防火墙服务（若打开了iptables，则需要开通3260端口）。selinux也要关闭！！ 2）服务端的操作记录（即ipsan-node01、ipsan-node02、ipsan-node03、ipsan-node04、ipsan-node05）
关闭iptbales防火墙 [root@ipsan-node01 ~]# systemctl stop firewalld.service [root@ipsan-node01 ~]# systemctl disable firewalld.service 关闭selinux [root@ipsan-node01 ~]# setenforce 0 setenforce: SELinux is disabled [root@ipsan-node01 ~]# getenforce Disabled [root@ipsan-node01 ~]# cat /etc/sysconfig/selinux ....... SELINUX=disabled 卸载之前挂载到/data下的数据盘，并重新格式化 [root@ipsan-node01 ~]# fdisk -l Disk /dev/vda: 42.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/ip-san/readme/</guid>
      <description>SAN 简单介绍 SAN，即存储区域网络（storage area network and SAN protocols），它是一种高速网络实现计算机与存储系统之间的数据传输。常见的分类是FC-SAN和IP-SAN两种。FC-SAN通过光纤通道协议转发scsi协议；IP-SAN通过TCP协议转发scsi协议，也就是IP 地址。存储设备是指一台或多台用以存储计算机数据的磁盘设备，通常指磁盘阵列，主要厂商EMC、日立等。
iSCSI（internet SCSI）技术由IBM公司研究开发，是一个供硬件设备使用的、可以在IP协议的上层运行的SCSI指令集，这种指令集合可以实现在IP网络上运行SCSI协议，使其能够在诸如高速千兆以太网上进行路由选择。iSCSI是一种新储存技术，它是将现有SCSI接口与以太网络(Ethernet)技术结合，使服务器可与使用IP网络的储存装置互相交换资料。
iSCSI是一种基于TCP/IP 的协议，用来建立和管理IP存储设备、主机和客户机等之间的相互连接，并创建存储区域网络（SAN）。SAN 使得SCSI 协议应用于高速数据传输网络成为可能，这种传输以数据块级别（block-level）在多个数据存储网络间进行。SCSI 结构基于C/S模式，其通常应用环境是：设备互相靠近，并且这些设备由SCSI 总线连接。
iSCSI 的主要功能是在TCP/IP 网络上的主机系统（启动器 initiator）和存储设备（目标器 target）之间进行大量数据的封装和可靠传输过程。
完整的iSCSI系统的拓扑结构如下：
iSCSI简单来说，就是把SCSI指令通过TCP/IP协议封装起来，在以太网中传输。iSCSI 可以实现在IP网络上传递和运行SCSI协议，使其能够在诸如高速千兆以太网上进行数据存取，实现了数据的网际传递和管理。基于iSCSI建立的存储区域网（SAN）与基于光纤的FC-SAN相比，具有很好的性价比。
iSCSI属于端到端的会话层协议，它定义的是SCSI到TCP/IP的映射（如下图），即Initiator将SCSI指令和数据封装成iSCSI协议数据单元，向下提交给TCP层，最后封装成IP数据包在IP网络上传输，到达Target后通过解封装还原成SCSI指令和数据，再由存储控制器发送到指定的驱动器，从而实现SCSI命令和数据在IP网络上的透明传输。它整合了现有的存储协议SCSI和网络协议TCP/IP，实现了存储与TCP/IP网络的无缝融合。在本篇中，将把发起器Initiator称为客户端，将目标器Target称为服务端以方便理解。
存储网络（SAN）：SAN 是指存储设备相互连接且与一台服务器或一个服务器群相连的网络。其中的服务器用作 SAN 的接入点。在有些配置中，SAN 也与网络相连。SAN 中将特殊
交换机
当作连接设备。它们看起来很像常规的以太网络交换机，是 SAN 中的连通点。SAN 使得在各自网络上实现相互通信成为可能，同时并带来了很多有利条件。
SAN英文全称：Storage Area Network，即
存储区域网络
。它是一种通过光纤
集线器
、光纤
路由器
、
光纤交换机
等连接设备将
磁盘阵列
、磁带等存储设备与相关服务器连接起来的高速专用子网。
SAN由三个基本的组件构成：接口（如SCSI、
光纤通道
、ESCON等）、连接设备（交换设备、
网关
、路由器、集线器等）和通信控制协议（如IP和SCSI等）。这三个组件再加上附加的存储设备和独立的SAN服务器，就构成一个SAN系统。SAN提供一个专用的、高可靠性的基于光通道的存储网络，SAN允许独立地增加它们的存储容量，也使得管理及集中控制（特别是对于全部存储设备都集群在一起的时候）更加简化。而且，光纤接口提供了10 km的连接长度，这使得物理上分离的远距离存储变得更容易.
网络存储通信中使用到的相关技术和协议包括 SCSI 、RAID 、iSCSI 以及光纤信道。一直以来 SCSI 支持高速、可靠的
数据存储
。RAID（独立磁盘冗余阵列）指的是一组标准，提供改进的性能和/或磁盘容错能力。光纤信道是一种提供存储设备相互连接的技术，支持高速通信（将来可以达到 10Gbps ）。与传统存储技术，如 SCSI 相比，光纤信道也支持较远距离的设备相互连接。iSCSI 技术支持通过 IP 网络实现存储设备间双向的数据传输。其实质是使 SCSI 连接中的数据连续化。通过 iSCSI，网络存储器可以应用于包含 IP 的任何位置。而作为 Internet 的主要元素，IP 几乎无所不在。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/nas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/nas/</guid>
      <description>NAS </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/gai-shu/readme/</guid>
      <description>网络存储 Direc(DirectAttachedStorage)，全称为直接连接附加存储，采用DAS的方式可以很简单的实现平台的容量扩容，同时对数据可以提供多种RAlD级别的保护。
DAS直联存储
采用DAS方式，在视频存储单元上部署相关的HBA卡。用于跟后端的存储设备建立数据通道。前端的视频存储单元可以是DVR，也可以是视频存储服务器。其通道可以采用光纤、IP网线、SAS线缆甚至于USB。采用DAS方式并不能同时支持很多视频存储服务单元同时接入，而且其扩容能力严重依赖所选择的存储设备自身的扩容能力。所以在大型数字视频监控系统中，应用DAS存储方式将造成系统维护难度的极大提升。正是由于DAS存储的这些特点，所以这种存储方式一般应用于对于DVR的扩容或者小型数字视频监控项目中。
NAS网络附加存储
NAS(NetworkAttachedStorage)。全称为网络附加存储，是一种专业的网络文件存储及文件备份设备。同时NAS对数据可以提供多种RAID级别的保护。NAS设备和多台视频存储服务单元均通过IP网络进行连接，按照TCP/IP协议进行通信，以文件的I/O(输入/输出)方式进行数据传输。一个NAS单元包括核心处理器，文件服务管理工具，一个或者多个的硬盘驱动器用于数据的存储。
采用NAS方式可以同时支持多个主机端同时进行读写，具备非常优秀的共享性能和扩展能力;同时NAS可以应用在复杂的网络环境中。部署也非常灵活。
但由于NAS采用CIF/NFS协议进行数据的文件级传输，所以网络开销非常大，特别是在写入数据时带宽的利用率一般只有20%。所以目前NAS一般应用于小型的网络数字视频监控系统中或者只是用于部分数据的共享存储。
SAN存储区域网络
SAN(StorageAreaNetwork)，全称为存储区域网络，通过交换机等连接设备将磁盘阵列与相关服务器连接起来的高速专用子网。同时SAN对数据可以提供多种RAID级别的保护。SAN提供了一个专用的、高可靠性的存储网络。允许独立地增加它们的存储容量，也使得管理及集中控制更加简化。正是由于这些特点，SAN架构特别适合于大型网络数字视频监控系统的存储应用，可以应对上千、上万个前端监控点的存储。目前SAN主要分为FC-SAN(光纤存储区域网络)和IP-SAN(以太网存储区域网络)。它们之间的区别是连接线路以及使用数据传输协议的不同。虽然FC-SAN由于采用专用协议可以保证传输时更加稳定、高效，但其部署方式、构建成本均较之IP-SAN高出很多，所以目前在大型网络数字视频监控系统中更多采用的是IP-SAN架构。
世博国家场馆采取的是给予IP网络的SAN存储架构，全面兼容前端网络数字产品的直接视频录入。针对视频监控这个特殊的行业模式，在系统平台层面上进行优化，前端数字编码设备将数字化后的视频数据直接写入IP-SAN设备中。充分利用系统架构，有效减少设备数量，降低整体系统的管理难度、维护强度，以及能源消耗。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/mgfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/mgfs/</guid>
      <description>mgfs MogileFS是一个开源的分布式文件存储系统，由LiveJournal旗下的Danga Interactive公司开发。Danga团队开发了包括 Memcached、MogileFS、Perlbal 等多个知名的开源项目。目前使用MogileFS 的公司非常多，如日本排名先前的几个互联公司及国内的yupoo(又拍)、digg、豆瓣、1号店、大众点评、搜狗和安居客等，分别为所在的组织或公司管理着海量的图片。
MogileFS由3个部分组成： (1) server：主要包括mogilefsd和mogstored两个应用程序。mogilefsd实现的是tracker，它通过数据库来保存元数据信息，包括站点domain、class、host等；mogstored是存储节点(store node)，它其实是个WebDAV服务，默认监听在7500端口，接受客户端的文件存储请求。在MogileFS安装完后，要运行mogadm工具将所有的store node注册到mogilefsd的数据库里，mogilefsd会对这些节点进行管理和监控。 (2) utils（工具集）：主要是MogileFS的一些管理工具，例如mogadm等。 (3) 客户端API：MogileFS的客户端API很多，例如Perl、PHP、Java、Python等，用这个模块可以编写客户端程序，实现文件的备份管理功能等。
存储主机(节点) 这个是 MogileFS 存储文件存放在这些机器上,也是 mogstored 节点,也叫 Storage Server,一台存储主要都要启动一个 mogstored 服务.扩容就是增加这些机器.
设备(device) 一个存储节点,以就是上面的主机,可以有多个 device, 就是用来存放文件的目录(例如挂载的目录),每个设备都有一个设备id,需要在 mogstored 的配置文件中的 docroot 配置的项目 指定的目录下面创建相应的设备的目录,目录名为 $docroot/dev$id,设备是不能删除的.只能将其设备的状态的值置为dead,当一个设备 dead 之后,就真的 dead了,里面的数据也无法恢复了,且这个dead了的设备的 id 也不能再用.
yum -y install make gcc unzip perl-DBD-MySQL perl perl-CPAN perl-YAML perl-Time-HiRes cpan App::cpanminus MogileFS::Server MogileFS::Utils IO::AIO IO::WrapTie Danga::Socket
2 cpanm安装 wget http://xrl.us/cpanm -O /usr/bin/cpanm; sudo chmod +x /usr/bin/cpanm
cpanm DBD::mysql cpanm MogileFS::Server cpanm MogileFS::Utils cpanm MogileFS::Client 所有的perl程序可以编译运行：</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/network-storage/ip-san/images/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/network-storage/ip-san/images/readme/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/she-bei/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/she-bei/</guid>
      <description>存储设备 存储设备是用于储存信息的设备，通常是将信息数字化后再以利用电、磁或光学等方式的媒体加以存储。
常见设备 1、利用电能方式存储信息的设备如：各式存储器，如RAM、ROM等
2、利用磁能方式存储信息的设备如：硬盘、软盘（已经淘汰）、磁带、磁芯存储器、磁泡存储器（磁泡存储器在1970年代出现，但是在1980年代硬盘价格急剧下降的情况下未能获得商业上的成功。），U盘
3、利用光学方式存储信息的设备如：CD或DVD
4、利用磁光方式存储信息的设备如：MO（磁光盘）
5、利用其他物理物如纸卡、纸带等存储信息的设备如：打孔卡、打孔带、绳结等
6、专用存储系统：用于数据备份或容灾的专用信息系统，利用高速网络进行大数据量存储信息的设备。
存储服务器通常是独立的单元。有的时候它们会被设计成4U机架式。或者，它们也可以由两个箱子组成——一个存储单元以及一个位于附近的服务器。然后两个箱子可以并行地安装在机柜中。像Sun StorEdge 3120 存储单元和SunFire X4100服务器，就可以合并为一个存储服务器并放置在一个机柜中。
那么，除了额外的磁盘外，存储服务器还有什么不同或独特的地方呢?在很多情况下，存储服务器会携带一大堆的特殊服务，包括存储管理软件、保证高灵活性的额外硬件、RAID配置类型，以及确保更多桌面使用者与之连接的额外网络连接等。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/chapter1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/chapter1/</guid>
      <description>存储 存储的内容比较广泛和零散，概念比较多，下面我就分享一下学习存储的心得
一、基本概念 1、什么是数据及数据与信息的区别：数据可以理解为对所有事物的数字表示，信息是经过加工处理的数据，它是在数据层面上经过
提炼得出来的有价值的数据。其实这两个概念都比较简单，但是非常重要。
2、了解了数据及信息的定义后，大家会想这么多的数据哪些是有用的呢？数据是否有价值是谁来决定的呢？带着这些问题，我们看一下
三个名词，分别是：RPO、RTO、和 COT，如果仅仅对这3个名词进行解释的话效果并不好，会让你联想为死记硬背的概念。所以，我就用通俗易懂的话语来说一下三个名词的作用：RPO的作用是统计发生灾难时我到底丢失了多少数据；RTO的作用是我想要恢复丢失的数据需要多长时间；COT的作用是灾难期间我公司每小时损失的数据成本是多少。有兴趣的话，大家可以去百度一下这三个名词缩写的具体含义。
3、第三个概念是现在比较流行的即ICT，ICT=IT+CT 就是信息技术与通信技术的融合。这个也是华为未来的发展方向。
4、最后说一下ICT的基础架构，其实可以理解为存储的架构，我们可以简单的理解为：主机+网络+存储，随着学习的深入以后会慢慢的丰富这个架构。
基本定义说完以后接下来说一下现在存储里面比较流行的三种解决方案，虽然很多人都听过但是真正能理解的人却不算太多。
二、网络存储技术：DAS、NAS、SAN
又是三个英文单词的缩写，如果想真正理解其含义及作用，必须进行单独的解析，下面我分别说一下三个名词。
1、DAS：直连式存储，可以理解为直接相连的存储方案。也是最早的存储设备，其他的不用多想知道它直连的就可以了。DAS一般分为两种类型：内部DAS和外部DAS，两者的区别是依据于连接位置区分的。处理器和存储设备放在一起的就是内部DAS，例如服务器；处理器和存储设备分离的就是外部DAS。DAS有个致命的缺陷就是连接距离最大不能超过25m，这个缺陷显示了DAS的发展。
2、NAS：网络附加存储，可以简单的理解为通过网络进行共享的技术，说白了就是共享服务，NAS常用的共享协议是NFS和CIFS，前者是类UNIX系统使用的协议，后者是Windows用的协议。NAS一般可以分为盘控一体和盘分离，盘指的是存储阵列，控指的是控制器，一般的NAS设备最少是双控（A控+B控）。
3、SAN：存储区域网络，它是DAS的升级版本，随着数据和设备的增加，DAS已经不能满足工作和生活的需要，所以出现了SAN，它弥补了DAS的不足。这三个技术的出现顺序是DAS早于SAN，SAN早于NAS，是不是有点意外。
第三部分介绍这三个技术中涉及和使用到的协议，也是大家比较熟悉的名词。
存储服务器
首先：磁盘阵列是存储介质，磁盘阵列其实是一大堆磁盘组成的一块大硬盘，通过做RAID实现并行读写，并通过数据校验保证数据的正确性和安全性。
而存储服务器分为文件服务器和备份服务器，文件服务器使磁盘阵列可以对外文件级的共享服务；备份服务器安装备份软件，可以对业务服务器产生的业务数据进行备份和归档，而备份方式、备份策略等都是可以在备份服务器上设定的。
存储服务器是用来备份重要文件等作用的，以防万一可以及时找回，而磁盘阵列是用在一个主机里安装2块硬盘，2块硬盘作相同的任务，但系统里只能看到一块，这是为了防止系统硬盘突然损坏，另一块可以马上起到备用作用。
三、存储常用的协议
1、ATA和SATA：其实这个定义可以不用记住，因为是比较老的东西，现在用的也不多，大家了解一下就可以了。
ATA是一种很早就出现的接口类型，后者是前者的改进即串行的ATA。
2、SCSI：这个出现的也是比较早，最先由IBM公司提出来，前期用于小型机的接口，全称叫“小型计算机系统接口”S（small）C（computer）S（system）I（interface）。后来又衍生出ISCSI。
3、SAS：就是串行的SCSI。
4、FC：光纤通道协议，它是光纤传输使用的协议。
接下来说一下市场上流行的几种硬盘，他们各自的特点及区别。
四、常用的硬盘
1、SATA盘：比较早的硬盘，现在基本不用了，特点是存储容量大，价格低；缺点是读取速度慢，不适合频繁的操作。
2、SAS盘：串行SCSI盘，现在比较流行的盘，特点是读取速度快，价格适中，性价比高。
3、SSD盘：固态硬盘，特点是读取速度极快，价格昂贵，生命周期短暂，性价比底。
4、NL-SAS：近线SAS，盘体采用的SATA，接口采用SAS，结合了两者的优点于一身，主要用在容量盘中。
下面介绍RAID技术。
五、传统RAID技术
RAID的定义：独立磁盘冗余阵列，它的作用主要是容灾和备份，容灾可以理解为容许灾难发生，备份就不用多说了，也可以理解为数据恢复技术还有就是保障数据的写入和读取的效率。
RAID根据业务不同的分为不同的等级：0、1、2、3、4、5、6、10、50等，其中2和4不常用，但是确实存在，剩下的
是比较常用的RAID级别，下面简单的说一下几个RAID的区别。
1、RAID0: 把数据同时写入2块硬盘，读取效率提升50%，没有备份盘，坏了就完了，不能恢复和重构数据，作用
仅仅是提高性能。一般单独做RAID0的很少。
2、RAID1：把一样的数据分别拷贝到两个盘上，这两个盘的数据完全一样，缺点是硬盘的利用率只有50%，优点是当一个盘坏掉，另一个盘可以正常使用。
3、RAID3：最少需要4个硬盘，其中一个硬盘作为校验盘，具有数据校验的功能，所有校验的信息都放到校验盘中。当某个数据
丢失后，通过其他数据和校验盘进行异或运算推出丢失的数据，优点：数据丢失后可以恢复，缺点：需要额外的校验盘，还有一点就是存在“写惩罚”。
4、RAID5：RAID3的升级版，去掉了校验盘，所有校验任务都平均分配到各个盘中，可以理解为分布式存储，优点是数据恢复速度快，不需要校验盘；缺点是：每个盘都参与校验，硬盘使用频率高，损坏的几率也高。
5、RAID6：继续升级，有2块校验盘，有2种校验模式，例如：P+Q校验 和 DP校验，优点是其他RAID不能比拟的，它允许同时损坏2块硬盘，是不是很给力。
6、最后这个其实就2个组合：RAID10：先做RAID1，再做RAID0，这种模式性能和效率都很高比较流行。
RAID50：先做RAID5，再做RAID0
下面说一下两个概念。
六、其他知识
1、大数据
SNIA（全球网络存储工业协会）对大数据下的定义是，在最强大的计算平台上都无法对全部数据进行有效的处理的数据集合。
提到数据不得不对其进行分类，我们可以简单的分为2类：结构化数据和非结构化数据，当然也有人分为三类即增加了半结构化数据，
在这里我们简单的分为2类即可。结构化数据是指用二维表可以逻辑的表示出来的数据，例如，Excel、数据库等。非结构化数据是不能用二维表逻辑的表示出来，例如，图片、视频、文档等。
2、云计算
云就是互联网，就是互联网计算。它有三种部署模式和三种商业模式，部署模式有：私有云、公有云和混合云，这里不详细的介绍。商业模式有：IAAS（基础架构就是一种服务）、PAAS（平台也算是一种服务）、SAAS（软件也是一种服务）综上所述云计算就是卖服务的，就像电力集团卖电一样。
七、RAID2.0+
上面说的RAID是传统RAID技术，随着技术的发展，主流厂商纷纷提出来新的RAID技术，RAID2.0+是华为推出的，它的核心思想是把数据平均分配到每个磁盘上，当读取数据的时候所有磁盘都参与进来，这样读取的速率就会大大的增加。
首先将所有磁盘都划分为单个的chunk，多个chunk组成一个chunk组（ckg）。从ckg中再细分成extent，根据磁盘类型的不同，extent的大小也不一样。SAS盘和SSD盘中extent的大小为64M，NL-SAS盘extent的大小为256M，所有的extent组成一个卷，从卷中再映射出Lun，呈献给用户的就是一个个的Lun，一个Lun就是一个逻辑卷。其实在extent中还可以继续划分为更小的单位即grain，它是thin Lun的组成单位。
7。云存储服务
8分布式文件系统
什么是云存储? 首先，需要了解云存储的定义。
云存储是基于云计算（Cloud Computing）建立起来的一个网络存储技术即与计算的存储部分，将网络中的不同设备通过应用程序连接起来，进行协同工作，对外提供数据存储和业务访问。总体来说，云存储已经成为一种服务，即为用户提供存储和访问服务。
云存储作为一种服务，通过网络提供给用户。用户们可以通过以下几种方式来使用存储，并按照使用来付费。</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/cun-chu-lei-xing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/cun-chu-lei-xing/</guid>
      <description>存储类型 原文：https://yq.aliyun.com/articles/85619
1.存储引擎 1.1 Hash Table  1.1.1 dbm (database manager)  https://en.wikipedia.org/wiki/Dbm
The dbm library stores arbitrary data by use of a single key (a primary key) in fixed-size buckets and uses hashing techniques to enable fast retrieval of the data by key.
1.2 btree  1.2.1 berkerlydb  https://en.wikipedia.org/wiki/Berkeley_DB
http://baike.baidu.com/view/1281930.htm
Key/value数据模型
Berkeley DB最初开发的目的是以新的HASH访问算法来代替旧的hsearch函数和大量的dbm实现（如AT&amp;amp;T的dbm，Berkeley的 ndbm，GNU项目的gdbm）
oracle
Written in C, java
BTREE, HASH, QUEUE, RECNO storage
https://www.oracle.com/database/berkeley-db/db.html
http://www.oracle.com/technetwork/database/database-technologies/berkeleydb/overview/index.html
1.2.2 LMDB (Lightning Memory-Mapped Database)  LMDB is a Btree-based database management library modeled loosely on the BerkeleyDB API, but much simplified.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/leixing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/leixing/</guid>
      <description>leixing https://blog.csdn.net/enweitech/article/details/51445087
https://blog.csdn.net/liuaigui/article/details/17973039
   差异点\产品 ServerSAN 分布式NAS 分布式对象存储     接口协议 块（SCSI协议） 文件（NFS、CIFS协议） 对象（OpenStackSWIFT、Amazon S3）   时延 较低(&amp;lt;10ms左右) 中(10ms~100ms) 高（60ms以上）   一致性 强一致性 强一致性 最终一致性   应用场景 分布式云平台、数据仓库、虚拟机；客户为企业客户 文件共享，如媒资、医疗影像归档、卫星图片 云服务，如Amazon S3;客户为个人客户    ServerSAN
1, 适配云化环境，有良好的扩展伸缩能力
2，适配未来云化环境下的自动资源分发以及资源管理需求。
3，由于企业的云数据中心主要承载企业业务，因此对性能时延也有一定的要求，设计采用了支持IB组网，支持SSD cache加速等提高性能的设计。
4，数据需要有强一致性，任何时刻的数据必须保证一致性以及可靠性。（为了性能采用多副本跨界点分布等方式来保证数据可靠性）
5，由于数据分块以及跨节点的数据转发，并不适合大文件顺序流读写场景。
分布式NAS：
目标是面对高性能计算，大吞吐量的数据处理，大数据处理等企业存储需求。
1， 为了大吞吐量，支持高效IB互联等技术
2， 支持SSD作为cache加速等
3， 应用场景，媒资、HPC等场景都是使用NAS来做的，支持NFS/CIFS接口
对象存储：
1， 为了满足大并发，我们的节点就不能有主备等角色之分，而是每个节点都能处理业务，而不是内部进行转发
2，元数据采用链接地址的方式转嫁给最终个人用户去保存，减少元数据的结构，快速响应（serversan中这部分资源管理采用X86 CPU资源，并且在存储中保存元数据LUN）
超大规模数据管理能力（性能不下降）是Object存储相对于文件存储的最大优势。File Storage采用了树形结构对所有文件和目录进行管理，当文件或目录过多时，文件或目录的检索性能就会极大下降。Object Storage只有目录和对象两层结构，这种扁平化的结构即使对象数量达到百亿级别，对象的检索速度依然不会有大的变化。但对象存储接口是应用级接口，而不是系统级接口，因此传统应用迁移到对象存储时需要重新开发，这是对象存储规模应用的最大困难。
1， Object storage相对于file storage 核心差异有几点</description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/storage/readme/</guid>
      <description>浅谈存储 </description>
    </item>
    
    <item>
      <title></title>
      <link>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://aftree.github.io/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/linux/storage/summary/</guid>
      <description> 浅谈存储  存储 存储类型 leixing   存储设备 网络存储  DAS NAS SAN  IP-SAN FC-SAN     分布式文件系统  分布式 Ceph  硬件选择 概述 安装2 集群管理 日常操作命令 故障排除 安装 运维   GlusterFS  概述 Install Guide Administration Guide  Admin Admin2 更换故障Brick   数据的恢复机制(AFR)的说明   FastDFS  概述 部署 常用命令   MooseFS DRBD 调度 诗   云存储  </description>
    </item>
    
  </channel>
</rss>